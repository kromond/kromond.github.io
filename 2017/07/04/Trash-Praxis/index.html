<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>Trash Praxis - Kevin Romond</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="A collection of projects">
  

  <!--Author-->
  
      <meta name="author" content="Kevin Romond">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/styles.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics -->
  
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-64554420-1', 'auto');
        ga('send', 'pageview');
    </script>


</head>


  <body>
  	<div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/">Kevin Romond</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
        <li class="nav-item">
          <a class="nav-link" href="/about">About</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="/contact">Contact</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://www.instagram.com/kromond/">instagram</a>
        </li>
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			<div class="col-12 mb-4">
  <img class="img-fluid project-img" src="/assets/images/CaptureYT_3_coverimg.jpg" alt="Trash Praxis">
</div>
<div class="col-lg-4 col-12 pt-3 px-4 pr-lg-5">
  <h1>Trash Praxis</h1>
</div>
<div class="col-lg-8 col-12 pt-lg-3 mb-4 pl-lg-5 px-lg-0 px-4 portfolio-content">
  <h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>The initial motivation for this project was to investigate rapid content creation for virtual reality head mounted displays (VR HMD) using a game engine and motion capture technology.  </p>
<h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>The concept was to create VR content that could be distributed either as a high quality ‘room scale’ VR experience using desktop playback or as a 360 video distributed via YouTube for google cardboard/GearVR type display.  </p>
<p>For the proof of concept a very simple framework was used: something like a 2 person podcast or talkshow with computer generated visuals.  The talk show topic material would be casual discussion of media and culture between hosts Alfio Leotta and Raqi Syed, both academics in the area of Media studies at Victoria University of Wellington.  The project was undertaken as part of the VUW ‘Digital Futures’ research initiative.  The initial phase of this project was completed over the last quarter of 2016 and into the first quarter of 2017.</p>
<h3 id="Styling"><a href="#Styling" class="headerlink" title="Styling"></a>Styling</h3><p>This talk show format has an existing visual trope, the sofa and potted plants, table with coffee cups</p>
<p><img src="talkshowstyle.png" alt="talkshowstyle.png"></p>
<p>With this we would replace the humans with digital character creations, taking clues from the Muppets, particularly embracing design elements to support secondary motion- things like hair, feathers:</p>
<p><img src="muppetstyle.png" alt="muppetstyle.png"></p>
<p>Concepts were drafted, and characters built/dressed.  We chose to minimize digital asset creation effort by using free non-commercial assets or purchasing pre-made assets, then augmenting as needed.  The character puppets are from the Autodesk character builder.  </p>
<p><img src="costumeWip3.gif" alt="img1"> <img src="scWip.gif" alt="img2"></p>
<h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>The methodology would employ motion capture ‘Virtual Production’ techniques to capture the performance as completely as possible, then deliver the results with Unreal Engine 4.</p>
<h5 id="Game-Engine"><a href="#Game-Engine" class="headerlink" title="Game Engine"></a>Game Engine</h5><p>Unreal Engine 4 is a game engine from Epic Games well suited toward linear cinematic VR content creation.  It has many features to deliver a rich visual quality</p>
<h5 id="‘Virtual-Production’"><a href="#‘Virtual-Production’" class="headerlink" title="‘Virtual Production’"></a>‘Virtual Production’</h5><p>is a film making technique refined through the productions like the film ‘Avatar’ and ‘War for the Planet of the Apes’ whereby live action performances are captured using various technologies to be reconstructed into digital images of that performance.  </p>
<p><a href="https://www.youtube.com/watch?v=ZnrKKjaVxCA" target="_blank" rel="noopener">This video</a> shows some good examples and describes the technique. </p>
<h3 id="Capture"><a href="#Capture" class="headerlink" title="Capture"></a>Capture</h3><p>The motion capture was done using the 8 camera Optitrack system installed at VUW School of Design.<br>With the help of a team of engaged students we  collected an array of data from our performers including multichannel audio, reference video, motion capture data, and facial performance video.  With this data, we can reconstruct the event with computer graphics characters and present the result via Unreal Engine 4 in multiple formats allowing us to gage the result against the production effort for each format. </p>
<p>Here is an example from one of the motion capture sessions, captured with the Gear360 camera</p>
<iframe src="https://www.youtube.com/embed/HDHagPLOmwY" width="640" height="360" frameborder="0" allowfullscreen></iframe>
<hr>
<h3 id="Post-production"><a href="#Post-production" class="headerlink" title="Post-production"></a>Post-production</h3><p>The Post-production for this type of content is not particularly rapid and comes with technical challenges. Our aims have two formats for content delivery.  One is 360 degree video.  The other is geometry and animation delivered by game engine.  The former is derived from the latter.  Both have their drawbacks and merits, both qualitatively and in terms of production issues. This project proposed to deliver in both formats to provide empirical data on viable production methods for VR content. </p>
<p>Here is a very early sample of putting the data onto characters in Unreal Engine 4</p>
<iframe src="https://www.youtube.com/embed/lQlDdcQtojE" width="640" height="360" frameborder="0" allowfullscreen></iframe>
<h3 id="Faces"><a href="#Faces" class="headerlink" title="Faces"></a>Faces</h3><p>Clearly the lifeless faces are going to be limiting to viewer engagement.  In this next test, the characters were covered with fur and put into a woodland scene.</p>
<iframe src="https://www.youtube.com/embed/uwYXBlgDZNg" width="640" height="360" frameborder="0" allowfullscreen></iframe>
<p>Slightly more watchable, however it’s clear faces will be critical to the success of the piece.</p>
<p>Faces were a consideration from the start.  For phase 1 on of the project, I assembled a single helmet mounted camera from a bike helmet and goPro clone. </p>
<p><img src="helmetcam.png" alt="helmetcam.png"></p>
<p>We used this on Raqi.  For Alfio,we recorded a ‘witness camera’, locked off and tight on the head. We got reasonable data from these.</p>
<iframe src="https://www.youtube.com/embed/n4zpVPKpk7Q" width="640" height="360" frameborder="0" allowfullscreen></iframe>
<p>However, we did not firmly establish exactly <em>how</em> we could carry the facial performance in the game engine.  I spent some time looking into options including open source options for markerless facial tracking and how to use this data to manipulate a cg face.</p>
<p>I also trialled a commercial option.  These options all required a high quality facial performance rig in CG, which is not trivial to build particularly on the timeline we established for the phase 1 of this project.  Additionally, from the beginning I was very interested in using the video of the facial performance as a component in the final result.  I felt strongly this would convey subtlety and expression and help us avoid the uncanny valley.  This pointed toward using a face fitting 3d morphable model.  I looked at several projects on github, compiling several including <a href="https://github.com/patrikhuber/4dface" target="_blank" rel="noopener">this one</a> but was unable to get suitable results out_of_the_box and was unable to extend these on my own.  This area I’d like to find technical assistance with.</p>
<p>For the immediate term, I used a basic method for jaw motion on the male character and projected video on the female character.  Here is that result:</p>
<iframe src="https://www.youtube.com/embed/y6R-Jx9W5c0" width="640" height="360" frameborder="0" allowfullscreen></iframe>
<p>This version was able to be experienced with ‘room scale’ on the HTC Vive.  It was somewhat successful.  As you can see we chose to ditch the sofa and record our performers standing, as we felt there would be more motion and less issues with the motion capture due to marker occlusion.  The room was not well suited to recording audio.  We put acoustic treatment into the space in the form of bass trap panels, but it was insufficient and we have reverb in the recordings along with crossing on the mics.  This is time consuming to clean up, as was the mocap data.</p>
<p>There is much further work that could be done.s</p>
<h3 id="Next-Steps"><a href="#Next-Steps" class="headerlink" title="Next Steps"></a>Next Steps</h3><p>For this project to be successful, a solution for compelling, watchable faces would be the primary focus:</p>
<ul>
<li>build a facial performance methodology, considering options for 4d face fitting and incorporating the video footage with cg augmentation, something like snapchat filters in 3d space.</li>
</ul>
<p><img src="snapchatexamp.png" alt="snapchatexamp.png"></p>
<p>Other steps - </p>
<ul>
<li><p>Place characters into landscape and evaluate and resolve performance impact for room scale VR experience </p>
</li>
<li><p>Spatialize audio for room scale version</p>
</li>
<li><p>Render 360 video of experience for 360 YouTube version</p>
</li>
<li><p>Process/spatialize performer audio for YouTube version, consider additional music or atmospheric audio </p>
</li>
<li><p>Refine entire production process and deliver further episodes</p>
</li>
<li><p>Build a channel, content strategy, growth strategy</p>
</li>
</ul>
<hr>
<h4 id="Update-17-Oct-2017"><a href="#Update-17-Oct-2017" class="headerlink" title="Update 17 Oct 2017"></a>Update 17 Oct 2017</h4><p>I have met someone at the FMX conference and again at Siggraph earlier this year.  A very nice guy named <a href="http://www.mahmoudhesham.net/" target="_blank" rel="noopener">Mahmoud Hesham</a> who has done work in this area of facial performance capture and face fitting.  I have asked him if he would be willing to collaborate on this project.  After exchanging some data, he sent me these tests</p>
<p>Snapchat filter style augmentation in Unity:<br><img src="bowie.gif" alt="bowieface"></p>
<p>A test using the Alfio witness cam:<br><img src="alfioFaceTest.PNG" alt="alfioFaceTest.PNG"></p>
<p>We are continuing to discuss how we could work together on this.  In further updates, I have continued my engagement with VUW and set up for them a new mocap space at the Miramar Creative Center.  The space larger and has 12 cameras.  We successfully captured full finger motion and had significantly less issues overall using this space. I am very optimistic about the next chapter in this story!</p>

</div>


  		</div>
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  </body>
</html>
